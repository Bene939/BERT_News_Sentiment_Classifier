{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "BERT_News_Sentiment_Classifier.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyP+rPJwKrCufot+44hvKIY0",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Bene939/BERT_News_Sentiment_Classifier/blob/main/BERT_News_Sentiment_Classifier.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bdtwSL6npW25"
      },
      "source": [
        "!pip install transformers\n",
        "!pip install torch\n",
        "!pip install pandas\n",
        "!pip install pathlib\n",
        "!pip install sklearn\n",
        "!pip install numpy\n",
        "#!pip install simpletransformers"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "58IV4lV7plhd"
      },
      "source": [
        "from transformers import BertForSequenceClassification, AdamW, BertTokenizer, get_linear_schedule_with_warmup, Trainer, TrainingArguments\n",
        "import torch\n",
        "from torch.utils.data import DataLoader, RandomSampler, SequentialSampler, TensorDataset\n",
        "import pandas as pd\n",
        "from pathlib import Path\n",
        "import sklearn\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score, precision_recall_fscore_support\n",
        "import numpy as np\n",
        "from torch.nn import functional as F\n",
        "from collections import defaultdict\n",
        "import random\n",
        "\n",
        "#from simpletransformers.classification import ClassificationModel"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "txYiiXGwrjN2"
      },
      "source": [
        "#defining tokenizerm, model and optimizer\n",
        "tokenizer = BertTokenizer.from_pretrained('bert-base-cased')\n",
        "model = BertForSequenceClassification.from_pretrained('bert-base-cased', num_labels=3)\n",
        "\n",
        "\n",
        "if torch.cuda.is_available():\n",
        "  print(\"\\nUsing: \", torch.cuda.get_device_name(0))\n",
        "  device = torch.device('cuda')\n",
        "else:\n",
        "  print(\"\\nUsing: CPU\")\n",
        "  device = torch.device('cpu')\n",
        "model = model.to(device)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nGvkz7wTC3W0"
      },
      "source": [
        "#loading dataset\n",
        "labeled_dataset = \"news_headlines_sentiment.csv\"\n",
        "labeled_dataset_file = Path(labeled_dataset)\n",
        "file_loaded = False\n",
        "while not file_loaded:\n",
        "  if labeled_dataset_file.exists():\n",
        "    labeled_dataset = pd.read_csv(labeled_dataset_file)\n",
        "    file_loaded = True\n",
        "    print(\"Dataset Loaded\")\n",
        "  else:\n",
        "    print(\"File not Found\")\n",
        "print(labeled_dataset)\n",
        "\n",
        "#counting sentiments\n",
        "negative = 0\n",
        "neutral = 0\n",
        "positive = 0\n",
        "for idx, row in labeled_dataset.iterrows():\n",
        "  if row[\"sentiment\"] == 0:\n",
        "    negative += 1\n",
        "  elif row[\"sentiment\"] == 1:\n",
        "    neutral += 1\n",
        "  else:\n",
        "    positive += 1\n",
        "print(\"Unbalanced Dataset\")\n",
        "print(\"negative: \", negative)\n",
        "print(\"neutral: \", neutral)\n",
        "print(\"positive: \", positive)\n",
        "\n",
        "#balancing dataset to 1/3 per sentiment\n",
        "for idx, row in labeled_dataset.iterrows():\n",
        "  if row[\"sentiment\"] == 0:\n",
        "    if negative - neutral != 0:\n",
        "      index_name = labeled_dataset[labeled_dataset[\"news\"] == row[\"news\"]].index\n",
        "      labeled_dataset.drop(index_name, inplace=True)\n",
        "      negative -= 1\n",
        "  elif row[\"sentiment\"] == 2:\n",
        "    if positive - neutral != 0:\n",
        "      index_name = labeled_dataset[labeled_dataset[\"news\"] == row[\"news\"]].index\n",
        "      labeled_dataset.drop(index_name, inplace=True)\n",
        "      positive -= 1\n",
        "\n",
        "negative = 0\n",
        "neutral = 0\n",
        "positive = 0\n",
        "for idx, row in labeled_dataset.iterrows():\n",
        "  if row[\"sentiment\"] == 0:\n",
        "    negative += 1\n",
        "  elif row[\"sentiment\"] == 1:\n",
        "    neutral += 1\n",
        "  else:\n",
        "    positive += 1\n",
        "print(\"Balanced Dataset:\")\n",
        "print(\"negative: \", negative)\n",
        "print(\"neutral: \", neutral)\n",
        "print(\"positive: \", positive)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xatcG9i7bgBh"
      },
      "source": [
        "#loading phrase bank dataset\n",
        "phrase_bank_dataset = \"all-data.csv\"\n",
        "phrase_bank_dataset_file = Path(phrase_bank_dataset)\n",
        "file_loaded = False\n",
        "while not file_loaded:\n",
        "  if phrase_bank_dataset_file.exists():\n",
        "    phrase_bank_dataset = pd.read_csv(phrase_bank_dataset, encoding='latin-1')\n",
        "    phrase_bank_dataset = phrase_bank_dataset.values.tolist()\n",
        "    file_loaded = True\n",
        "    print(\"Dataset Loaded\")\n",
        "  else:\n",
        "    print(\"File not Found\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RwgOX4KPbs2Y"
      },
      "source": [
        "#correcting the format of phrase bank dataset\n",
        "phrase_dataset = pd.DataFrame(columns=[\"news\", \"sentiment\"])\n",
        "for ele in phrase_bank_dataset:\n",
        "  news = ele[1]\n",
        "  #converting sentiment text into numbers\n",
        "  sentiment = 0 if ele[0] == 'negative' else 1 if ele[0] == 'neutral' else 2\n",
        "  row = [news, sentiment]\n",
        "  phrase_dataset.loc[len(phrase_dataset)] = row\n",
        "print(phrase_dataset)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4y2v0HmPEqrY"
      },
      "source": [
        "#merge both datasets\n",
        "\"\"\"\n",
        "final_dataset = pd.DataFrame(columns=[\"news\", \"sentiment\"])\n",
        "for idx,row in phrase_dataset.iterrows():\n",
        "  final_dataset.loc[len(final_dataset)] = [row[\"news\"], row[\"sentiment\"]]\n",
        "for idx,row in labeled_dataset.iterrows():\n",
        "  final_dataset.loc[len(final_dataset)] = [row[\"news\"], row[\"sentiment\"]]\n",
        "print(final_dataset)\n",
        "\"\"\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lT0KNTS5NtT1"
      },
      "source": [
        "#custom dataset class\n",
        "\n",
        "class NewsSentimentDataset(torch.utils.data.Dataset):\n",
        "  def __init__(self, encodings, labels):\n",
        "        self.encodings = encodings\n",
        "        self.labels = labels\n",
        "\n",
        "  def __getitem__(self, idx):\n",
        "      item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
        "      item['labels'] = torch.tensor(self.labels[idx])\n",
        "      return item\n",
        "\n",
        "  def __len__(self):\n",
        "      return len(self.labels)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5eQ2BqPNNQhT"
      },
      "source": [
        "#method for tokenizing dataset list\n",
        "\n",
        "def tokenize_headlines(headlines, labels, tokenizer):\n",
        "\n",
        "  encodings = tokenizer.batch_encode_plus(\n",
        "      headlines,\n",
        "      add_special_tokens = True,\n",
        "      truncation = True,\n",
        "      padding = 'max_length',\n",
        "      return_attention_mask = True,\n",
        "      return_token_type_ids = True\n",
        "  )\n",
        "\n",
        "  dataset = NewsSentimentDataset(encodings, labels)\n",
        "  return dataset"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wqAQGCeVOlvh"
      },
      "source": [
        "#splitting dataset into training and validation set\n",
        "#TODO: split dataset into train-val-test .7-.1-.2\n",
        "#load news sentiment dataset\n",
        "\n",
        "#all_headlines = phrase_dataset['news'].tolist()\n",
        "#all_labels = phrase_dataset['sentiment'].tolist()\n",
        "\n",
        "all_headlines = labeled_dataset['news'].tolist()\n",
        "all_labels = labeled_dataset['sentiment'].tolist()\n",
        "\n",
        "train_headlines, val_headlines, train_labels, val_labels = train_test_split(all_headlines, all_labels, test_size=.2)\n",
        "\n",
        "val_dataset = tokenize_headlines(val_headlines, val_labels, tokenizer)\n",
        "train_dataset = tokenize_headlines(train_headlines, val_labels, tokenizer)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RxRTbWnnxepB"
      },
      "source": [
        "#data loader\n",
        "train_batch_size = 16\n",
        "val_batch_size = 8\n",
        "#alternative to shuffle:\n",
        "#sampler=RandomSampler(train_dataset)\n",
        "train_data_loader = DataLoader(train_dataset, batch_size = train_batch_size, shuffle=True)\n",
        "val_data_loader = DataLoader(val_dataset, batch_size = val_batch_size, sampler=SequentialSampler(val_dataset))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kd_KSFJq8Q9H"
      },
      "source": [
        "#optimizer and scheduler\n",
        "num_epochs = 1\n",
        "num_steps = len(train_data_loader) * num_epochs\n",
        "optimizer = AdamW(model.parameters(), lr=5e-5, eps=1e-8)\n",
        "scheduler = get_linear_schedule_with_warmup(optimizer, num_warmup_steps=num_steps*0.06, num_training_steps=num_steps)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "505u8ro2-K8R"
      },
      "source": [
        "#simple transformers training\n",
        "\"\"\"\n",
        "#simple transformers model\n",
        "args = {\n",
        "  \"output_dir\": \"outputs/\",\n",
        "    \"cache_dir\": \"cache_dir/\",\n",
        "\n",
        "    \"fp16\": False,\n",
        "    \"fp16_opt_level\": \"O1\",\n",
        "    \"max_seq_length\": 128,\n",
        "    \"train_batch_size\": 8,\n",
        "    \"gradient_accumulation_steps\": 1,\n",
        "    \"eval_batch_size\": 8,\n",
        "    \"num_train_epochs\": 1,\n",
        "    \"weight_decay\": 0,\n",
        "    \"learning_rate\": 5e-5,\n",
        "    \"adam_epsilon\": 1e-8,\n",
        "    \"warmup_ratio\": 0.06,\n",
        "    \"warmup_steps\": 0,\n",
        "    \"max_grad_norm\": 1.0,\n",
        "\n",
        "    \"logging_steps\": 50,\n",
        "    \"save_steps\": 2000,\n",
        "\n",
        "    \"overwrite_output_dir\": True,\n",
        "    \"reprocess_input_data\": False,\n",
        "\n",
        "    \"manual_seed\": 64,\n",
        "    \"n_gpu\": 1\n",
        "}\n",
        "model = ClassificationModel('bert', 'bert-base-cased', num_labels=3,use_cuda=True, args=args)\n",
        "train,eva = train_test_split(labeled_dataset,test_size = 0.2)\n",
        "\n",
        "train_df = pd.DataFrame({\n",
        "    'text': train['news'],\n",
        "    'label': train['sentiment']\n",
        "})\n",
        "\n",
        "eval_df = pd.DataFrame({\n",
        "    'text': eva['news'],\n",
        "    'label': eva['sentiment']\n",
        "})\n",
        "\n",
        "model.train_model(train_df)\n",
        "\n",
        "result, model_outputs, wrong_predictions = model.eval_model(eval_df)\n",
        "\n",
        "\n",
        "lst = []\n",
        "for arr in model_outputs:\n",
        "    lst.append(np.argmax(arr))\n",
        "true = eval_df['label'].tolist()\n",
        "predicted = lst\n",
        "print(predicted)\n",
        "print(true)\n",
        "sklearn.metrics.accuracy_score(true,predicted)\n",
        "\"\"\"\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "I23_etTkePaN"
      },
      "source": [
        "#training and evaluation\n",
        "seed_val = 64\n",
        "\n",
        "random.seed(seed_val)\n",
        "np.random.seed(seed_val)\n",
        "torch.manual_seed(seed_val)\n",
        "torch.cuda.manual_seed_all(seed_val)\n",
        "\n",
        "for epoch in range(num_epochs):\n",
        "\n",
        "  print(\"\\n###################################################\")\n",
        "  print(\"Epoch: {}/{}\".format(epoch+1, num_epochs))\n",
        "  print(\"###################################################\\n\")\n",
        "\n",
        "  #training phase\n",
        " \n",
        "  average_train_loss = 0\n",
        "  average_train_acc = 0\n",
        "  model.train() \n",
        "  for step, batch in enumerate(train_data_loader):\n",
        "      \n",
        "      \n",
        "      input_ids = batch['input_ids'].to(device)\n",
        "      attention_mask = batch['attention_mask'].to(device)\n",
        "      labels = batch['labels'].to(device)\n",
        "      token_type_ids = batch['token_type_ids'].to(device)\n",
        "\n",
        "\n",
        "      outputs = model(input_ids=input_ids, attention_mask=attention_mask, token_type_ids = token_type_ids)\n",
        "\n",
        "      loss = F.cross_entropy(outputs[0], labels)\n",
        "      average_train_loss += loss\n",
        "\n",
        "      if step % 40 == 0:\n",
        "        print(\"Training Loss: \", loss)\n",
        "\n",
        "      logits = outputs[0].detach().cpu().numpy()\n",
        "      label_ids = labels.to('cpu').numpy()\n",
        "\n",
        "      average_train_acc += sklearn.metrics.accuracy_score(label_ids, np.argmax(logits, axis=1))\n",
        "      print(\"predictions: \",np.argmax(logits, axis=1))\n",
        "      print(\"labels:      \",label_ids)\n",
        "      print(\"#############\")\n",
        "      optimizer.zero_grad()\n",
        "      loss.backward()\n",
        "      #maximum gradient clipping\n",
        "      torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
        "      \n",
        "      optimizer.step()\n",
        "      scheduler.step()\n",
        "      model.zero_grad()\n",
        "\n",
        "  average_train_loss = average_train_loss / len(train_data_loader)\n",
        "  average_train_acc = average_train_acc / len(train_data_loader)\n",
        "  print(\"======Average Training Loss: {:.5f}======\".format(average_train_loss))\n",
        "  print(\"======Average Training Accuracy: {:.2f}%======\".format(average_train_acc*100))\n",
        "\n",
        "  #validation phase\n",
        "  average_val_loss = 0\n",
        "  average_val_acc = 0\n",
        "  model.eval()\n",
        "  for step,batch in enumerate(val_data_loader):\n",
        "    input_ids = batch['input_ids'].to(device)\n",
        "    attention_mask = batch['attention_mask'].to(device)\n",
        "    labels = batch['labels'].to(device)\n",
        "    token_type_ids = batch['token_type_ids'].to(device)\n",
        "\n",
        "    pred = []\n",
        "    with torch.no_grad():\n",
        "      \n",
        "\n",
        "      outputs = model(input_ids=input_ids, attention_mask=attention_mask, token_type_ids=token_type_ids)\n",
        "\n",
        "      loss = F.cross_entropy(outputs[0], labels)\n",
        "      average_val_loss += loss\n",
        "\n",
        "      logits = outputs[0].detach().cpu().numpy()\n",
        "      label_ids = labels.to('cpu').numpy()\n",
        "      print(\"predictions: \",np.argmax(logits, axis=1))\n",
        "      print(\"labels:      \",label_ids)\n",
        "      print(\"#############\")\n",
        "\n",
        "      average_val_acc += sklearn.metrics.accuracy_score(label_ids, np.argmax(logits, axis=1))\n",
        "\n",
        "  average_val_loss = average_val_loss / len(val_data_loader)\n",
        "  average_val_acc = average_val_acc / len(val_data_loader)\n",
        "\n",
        "  print(\"======Average Validation Loss: {:.5f}======\".format(average_val_loss))\n",
        "  print(\"======Average Validation Accuracy: {:.2f}%======\".format(average_val_acc*100))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KFT5MM7HrHdx"
      },
      "source": [
        "#training and evaluation with trainer moduel from huggingfaces\n",
        "\"\"\"\n",
        "def compute_metrics(pred):\n",
        "    labels = pred.label_ids\n",
        "    preds = pred.predictions.argmax(-1)\n",
        "    precision, recall, f1, _ = precision_recall_fscore_support(labels, preds, average='micro')\n",
        "    acc = accuracy_score(labels, preds)\n",
        "    return {\n",
        "        'accuracy': acc,\n",
        "        'f1': f1,\n",
        "        'precision': precision,\n",
        "        'recall': recall\n",
        "    }\n",
        "\n",
        "training_args = TrainingArguments(\n",
        "    output_dir='./results',          # output directory\n",
        "    num_train_epochs=1,              # total number of training epochs\n",
        "    per_device_train_batch_size=8,  # batch size per device during training\n",
        "    per_device_eval_batch_size=8,   # batch size for evaluation\n",
        "    warmup_steps=0,                # number of warmup steps for learning rate scheduler\n",
        "    weight_decay=0,               # strength of weight decay\n",
        "    logging_dir='./logs',            # directory for storing logs\n",
        "    logging_steps=10,\n",
        ")\n",
        "\n",
        "\n",
        "trainer = Trainer(\n",
        "    model=model,                         # the instantiated ðŸ¤— Transformers model to be trained\n",
        "    args=training_args,                  # training arguments, defined above\n",
        "    train_dataset=train_dataset,         # training dataset\n",
        "    eval_dataset=val_dataset  ,          # evaluation dataset\n",
        "    compute_metrics=compute_metrics           \n",
        ")\n",
        "\n",
        "trainer.train()\n",
        "trainer.evaluate()\n",
        "\"\"\""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}