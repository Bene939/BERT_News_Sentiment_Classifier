{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "BERT_News_Sentiment_Classifier.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Bene939/BERT_News_Sentiment_Classifier/blob/main/BERT_News_Sentiment_Classifier.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bdtwSL6npW25",
        "outputId": "50e95fd2-6bca-41af-8140-410dcf7b3831",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "!pip install transformers\n",
        "!pip install torch\n",
        "!pip install pandas\n",
        "!pip install pathlib\n",
        "!pip install sklearn\n",
        "!pip install numpy"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting transformers\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/3a/83/e74092e7f24a08d751aa59b37a9fc572b2e4af3918cb66f7766c3affb1b4/transformers-3.5.1-py3-none-any.whl (1.3MB)\n",
            "\u001b[K     |████████████████████████████████| 1.3MB 19.1MB/s \n",
            "\u001b[?25hRequirement already satisfied: packaging in /usr/local/lib/python3.6/dist-packages (from transformers) (20.4)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.6/dist-packages (from transformers) (4.41.1)\n",
            "Collecting sacremoses\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/7d/34/09d19aff26edcc8eb2a01bed8e98f13a1537005d31e95233fd48216eed10/sacremoses-0.0.43.tar.gz (883kB)\n",
            "\u001b[K     |████████████████████████████████| 890kB 51.0MB/s \n",
            "\u001b[?25hRequirement already satisfied: requests in /usr/local/lib/python3.6/dist-packages (from transformers) (2.23.0)\n",
            "Collecting tokenizers==0.9.3\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/4c/34/b39eb9994bc3c999270b69c9eea40ecc6f0e97991dba28282b9fd32d44ee/tokenizers-0.9.3-cp36-cp36m-manylinux1_x86_64.whl (2.9MB)\n",
            "\u001b[K     |████████████████████████████████| 2.9MB 27.4MB/s \n",
            "\u001b[?25hRequirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.6/dist-packages (from transformers) (2019.12.20)\n",
            "Collecting sentencepiece==0.1.91\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/d4/a4/d0a884c4300004a78cca907a6ff9a5e9fe4f090f5d95ab341c53d28cbc58/sentencepiece-0.1.91-cp36-cp36m-manylinux1_x86_64.whl (1.1MB)\n",
            "\u001b[K     |████████████████████████████████| 1.1MB 42.0MB/s \n",
            "\u001b[?25hRequirement already satisfied: protobuf in /usr/local/lib/python3.6/dist-packages (from transformers) (3.12.4)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.6/dist-packages (from transformers) (3.0.12)\n",
            "Requirement already satisfied: dataclasses; python_version < \"3.7\" in /usr/local/lib/python3.6/dist-packages (from transformers) (0.7)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from transformers) (1.18.5)\n",
            "Requirement already satisfied: pyparsing>=2.0.2 in /usr/local/lib/python3.6/dist-packages (from packaging->transformers) (2.4.7)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from packaging->transformers) (1.15.0)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers) (7.1.2)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers) (0.17.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (2020.6.20)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (1.24.3)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (3.0.4)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (2.10)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.6/dist-packages (from protobuf->transformers) (50.3.2)\n",
            "Building wheels for collected packages: sacremoses\n",
            "  Building wheel for sacremoses (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for sacremoses: filename=sacremoses-0.0.43-cp36-none-any.whl size=893257 sha256=f56cd04ec5435894e11f5784e9c06c383988c573888e0a0c5b90160cdf1e62d1\n",
            "  Stored in directory: /root/.cache/pip/wheels/29/3c/fd/7ce5c3f0666dab31a50123635e6fb5e19ceb42ce38d4e58f45\n",
            "Successfully built sacremoses\n",
            "Installing collected packages: sacremoses, tokenizers, sentencepiece, transformers\n",
            "Successfully installed sacremoses-0.0.43 sentencepiece-0.1.91 tokenizers-0.9.3 transformers-3.5.1\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.6/dist-packages (1.7.0+cu101)\n",
            "Requirement already satisfied: dataclasses in /usr/local/lib/python3.6/dist-packages (from torch) (0.7)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from torch) (1.18.5)\n",
            "Requirement already satisfied: future in /usr/local/lib/python3.6/dist-packages (from torch) (0.16.0)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.6/dist-packages (from torch) (3.7.4.3)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.6/dist-packages (1.1.4)\n",
            "Requirement already satisfied: pytz>=2017.2 in /usr/local/lib/python3.6/dist-packages (from pandas) (2018.9)\n",
            "Requirement already satisfied: python-dateutil>=2.7.3 in /usr/local/lib/python3.6/dist-packages (from pandas) (2.8.1)\n",
            "Requirement already satisfied: numpy>=1.15.4 in /usr/local/lib/python3.6/dist-packages (from pandas) (1.18.5)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.6/dist-packages (from python-dateutil>=2.7.3->pandas) (1.15.0)\n",
            "Requirement already satisfied: pathlib in /usr/local/lib/python3.6/dist-packages (1.0.1)\n",
            "Requirement already satisfied: sklearn in /usr/local/lib/python3.6/dist-packages (0.0)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.6/dist-packages (from sklearn) (0.22.2.post1)\n",
            "Requirement already satisfied: scipy>=0.17.0 in /usr/local/lib/python3.6/dist-packages (from scikit-learn->sklearn) (1.4.1)\n",
            "Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.6/dist-packages (from scikit-learn->sklearn) (0.17.0)\n",
            "Requirement already satisfied: numpy>=1.11.0 in /usr/local/lib/python3.6/dist-packages (from scikit-learn->sklearn) (1.18.5)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (1.18.5)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "58IV4lV7plhd"
      },
      "source": [
        "from transformers import BertModel, DistilBertModel, BertForSequenceClassification, AdamW, BertTokenizer, get_linear_schedule_with_warmup, Trainer, TrainingArguments\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.utils.data import DataLoader, RandomSampler, SequentialSampler, TensorDataset\n",
        "import pandas as pd\n",
        "from pathlib import Path\n",
        "import sklearn\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score, precision_recall_fscore_support\n",
        "import numpy as np\n",
        "from torch.nn import functional as F\n",
        "from collections import defaultdict\n",
        "import random\n",
        "import os"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "txYiiXGwrjN2",
        "outputId": "d1b6bed5-3f8a-4ab1-cfdf-6cca02cf8254",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "# setting device as gpu or cpu\n",
        "\n",
        "if torch.cuda.is_available():\n",
        "  print(\"\\nUsing: \", torch.cuda.get_device_name(0))\n",
        "  device = torch.device('cuda')\n",
        "else:\n",
        "  print(\"\\nUsing: CPU\")\n",
        "  device = torch.device('cpu')"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Using:  Tesla T4\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nGvkz7wTC3W0",
        "outputId": "88adeed0-13ce-4f5e-dd87-73f46354f370",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "# loading dataset\n",
        "\n",
        "labeled_dataset = \"news_headlines_sentiment.csv\"\n",
        "labeled_dataset_file = Path(labeled_dataset)\n",
        "file_loaded = False\n",
        "while not file_loaded:\n",
        "  if labeled_dataset_file.exists():\n",
        "    labeled_dataset = pd.read_csv(labeled_dataset_file)\n",
        "    file_loaded = True\n",
        "    print(\"Dataset Loaded\")\n",
        "  else:\n",
        "    print(\"File not Found\")\n",
        "print(labeled_dataset)"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Dataset Loaded\n",
            "                                                   news  sentiment\n",
            "0     UPDATE 3-Brazil economy back to 2009 size afte...          1\n",
            "1     GLOBAL MARKETS-Manufacturing data lifts stocks...          2\n",
            "2     TREASURIES-Yields move higher after U.S. manuf...          0\n",
            "3     UPDATE 2-Dollar weakness lifts pound to 8-mont...          0\n",
            "4     UPDATE 1-U.S. House Oversight Committee to sub...          1\n",
            "...                                                 ...        ...\n",
            "7995  Trian Investment in Comcast Fuels Debate on Br...          1\n",
            "7996                               Is Roku Stock a Buy?          2\n",
            "7997                10 Most Profitable TV Shows in 2020          0\n",
            "7998  Comcasts Amy Banse Transitions to Senior Advis...          2\n",
            "7999  Comcast and REVOLT Sign Agreement to Expand th...          0\n",
            "\n",
            "[8000 rows x 2 columns]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xatcG9i7bgBh",
        "outputId": "2344a2eb-1afa-42ed-ae79-990c18e3bdc5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "# loading phrase bank dataset and correcting format\n",
        "\n",
        "phrase_bank_dataset = \"all-data.csv\"\n",
        "phrase_bank_dataset_file = Path(phrase_bank_dataset)\n",
        "file_loaded = False\n",
        "while not file_loaded:\n",
        "  if phrase_bank_dataset_file.exists():\n",
        "    phrase_dataset = pd.read_csv(phrase_bank_dataset, encoding='latin-1', names=[\"sentiment\", \"news\"])\n",
        "    phrase_dataset = phrase_dataset[[\"news\", \"sentiment\"]]\n",
        "    phrase_dataset[\"sentiment\"].replace(['positive', 'negative', 'neutral'], [0,1,2], inplace=True)\n",
        "    file_loaded = True\n",
        "    print(\"Dataset Loaded\")\n",
        "  else:\n",
        "    print(\"File not Found\")\n",
        "print(phrase_dataset)"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Dataset Loaded\n",
            "                                                   news  sentiment\n",
            "0     According to Gran , the company has no plans t...          2\n",
            "1     Technopolis plans to develop in stages an area...          2\n",
            "2     The international electronic industry company ...          1\n",
            "3     With the new production plant the company woul...          0\n",
            "4     According to the company 's updated strategy f...          0\n",
            "...                                                 ...        ...\n",
            "4841  LONDON MarketWatch -- Share prices ended lower...          1\n",
            "4842  Rinkuskiai 's beer sales fell by 6.5 per cent ...          2\n",
            "4843  Operating profit fell to EUR 35.4 mn from EUR ...          1\n",
            "4844  Net sales of the Paper segment decreased to EU...          1\n",
            "4845  Sales in Finland decreased by 10.5 % in Januar...          1\n",
            "\n",
            "[4846 rows x 2 columns]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4y2v0HmPEqrY",
        "outputId": "d69b964d-9b5d-4dc2-abaa-4b3f0d17d0ee",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "# merge both datasets\n",
        "merged_dataset = pd.concat([phrase_dataset, labeled_dataset], axis=0)\n",
        "print(merged_dataset)"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "                                                   news  sentiment\n",
            "0     According to Gran , the company has no plans t...          2\n",
            "1     Technopolis plans to develop in stages an area...          2\n",
            "2     The international electronic industry company ...          1\n",
            "3     With the new production plant the company woul...          0\n",
            "4     According to the company 's updated strategy f...          0\n",
            "...                                                 ...        ...\n",
            "7995  Trian Investment in Comcast Fuels Debate on Br...          1\n",
            "7996                               Is Roku Stock a Buy?          2\n",
            "7997                10 Most Profitable TV Shows in 2020          0\n",
            "7998  Comcasts Amy Banse Transitions to Senior Advis...          2\n",
            "7999  Comcast and REVOLT Sign Agreement to Expand th...          0\n",
            "\n",
            "[12846 rows x 2 columns]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lT0KNTS5NtT1"
      },
      "source": [
        "# custom dataset class which returns the encodings and labels when called by the data loader\n",
        "# code is from https://huggingface.co/transformers/custom_datasets.html\n",
        "\n",
        "class NewsSentimentDataset(torch.utils.data.Dataset):\n",
        "  def __init__(self, encodings, labels):\n",
        "        self.encodings = encodings\n",
        "        self.labels = labels\n",
        "\n",
        "  def __getitem__(self, idx):\n",
        "      item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
        "      item['labels'] = torch.tensor(self.labels[idx])\n",
        "      return item\n",
        "\n",
        "  def __len__(self):\n",
        "      return len(self.labels)"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5eQ2BqPNNQhT"
      },
      "source": [
        "# expects data frame and tokenizer as input\n",
        "# returns encoded data as NewsSentimentDataset\n",
        "# token type ids are not included in encoding as they are only for Q&A to mark if its a question or an answer\n",
        "\n",
        "def tokenize_headlines(df, tokenizer):\n",
        "  encodings = tokenizer.batch_encode_plus(\n",
        "      df[\"news\"].tolist(),           # input the news headlines\n",
        "      add_special_tokens = True,     # special tokens added to mark beginning & end of sentence\n",
        "      truncation = True,             # make all sentences a fixed length\n",
        "      padding = 'max_length',        # pad with zeros to max length\n",
        "      return_attention_mask = True,  # include attention mask in encoding\n",
        "      return_tensors = 'pt'          # return as pytorch tensor\n",
        "  )\n",
        "\n",
        "  dataset = NewsSentimentDataset(encodings, df[\"sentiment\"].tolist())\n",
        "  return dataset"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wqAQGCeVOlvh",
        "outputId": "313b479a-a31e-4d6e-8fe9-11e246be6f6b",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "#splitting dataset into training and validation set\n",
        "#load news sentiment dataset\n",
        "#defining tokenizer\n",
        "\n",
        "tokenizer = BertTokenizer.from_pretrained('bert-base-cased')\n",
        "#Options for Dataset:\n",
        "# labeled_dataset = my own dataset\n",
        "# phrase_dataset = dataset from kaggle\n",
        "# merged_dataset = both datasets merged toegether\n",
        "#merged_dataset = pd.concat([phrase_dataset, labeled_train_data], axis=0) => for testing on unknown labeled date with merged dataset\n",
        "\n",
        "train_data, val_data = train_test_split(phrase_dataset, test_size=.2)\n",
        "merged_train_data, merged_val_data = train_test_split(merged_dataset, test_size=.2)\n",
        "labeled_train_data, labeled_val_data = train_test_split(labeled_dataset, test_size=.2)\n",
        "\n",
        "\n",
        "print(\"Train Dataset\\n\", train_data.reset_index(drop=True))\n",
        "print(\"Validation Dataset\\n\", val_data.reset_index(drop=True))\n",
        "\n",
        "train_dataset = tokenize_headlines(merged_train_data, tokenizer)\n",
        "val_dataset = tokenize_headlines(merged_val_data, tokenizer)"
      ],
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train Dataset\n",
            "                                                    news  sentiment\n",
            "0     ` Nordic infrastructure construction is one of...          2\n",
            "1     As a result of these negotiations the company ...          1\n",
            "2     Tikkurila has an interesting growth strategy ,...          2\n",
            "3     Net sales grew in the period to  x20ac 402 mil...          0\n",
            "4     After completion of the acquisition , Poyry 's...          2\n",
            "...                                                 ...        ...\n",
            "3871  Water Treatment Products In Australia Today , ...          2\n",
            "3872  No blind-spots coming from 1 vantage point all...          2\n",
            "3873  It 's `` finger-friendly '' , and to my opinio...          2\n",
            "3874  Shares of Standard Chartered ( STAN ) rose 1.2...          0\n",
            "3875  The employee negotiations are to address measu...          2\n",
            "\n",
            "[3876 rows x 2 columns]\n",
            "Validation Dataset\n",
            "                                                   news  sentiment\n",
            "0    A downloadable instruction sheet , instruction...          2\n",
            "1    Changes to the as-built models from the design...          2\n",
            "2    ALEXANDRIA , Va. , Oct. 23 -- Hans-Otto Scheck...          2\n",
            "3                It is expected to be online by 2011 .          2\n",
            "4    The announcement comes two weeks before a key ...          1\n",
            "..                                                 ...        ...\n",
            "965  In Q1 of 2010 , Bank of +_land 's net interest...          0\n",
            "966  The Nokia Music Store begins trading on Tuesda...          2\n",
            "967  The NTSB said investigators are set to conduct...          2\n",
            "968  Aspo serves demanding business-to-business cus...          2\n",
            "969  Part of the reductions will be made through re...          2\n",
            "\n",
            "[970 rows x 2 columns]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "I23_etTkePaN",
        "outputId": "86ec8aa1-767d-4f53-fb39-342c9f8e3a20",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "#TODO: Calculate per class accuracy\n",
        "#TODO: Try out different Loss function\n",
        "#TODO: Optimizer zero grad etc necessary?\n",
        "#TODO: Cased vs Uncased\n",
        "\n",
        "model = BertForSequenceClassification.from_pretrained('bert-base-cased', num_labels=3)\n",
        "model = model.to(device)\n",
        "#data loader\n",
        "train_batch_size = 8\n",
        "val_batch_size = 8\n",
        "\n",
        "train_data_loader = DataLoader(train_dataset, batch_size = train_batch_size, sampler=RandomSampler(train_dataset))\n",
        "val_data_loader = DataLoader(val_dataset, batch_size = val_batch_size, sampler=SequentialSampler(val_dataset))\n",
        "\n",
        "#optimizer and scheduler\n",
        "num_epochs = 1\n",
        "num_steps = len(train_data_loader) * num_epochs\n",
        "optimizer = AdamW(model.parameters(), lr=5e-5, eps=1e-8)\n",
        "#makes learning rate increase during warum up steps and decrease linearly during training.\n",
        "scheduler = get_linear_schedule_with_warmup(optimizer, num_warmup_steps=0, num_training_steps=num_steps)\n",
        "\n",
        "#training and evaluation\n",
        "seed_val = 64\n",
        "random.seed(seed_val)\n",
        "np.random.seed(seed_val)\n",
        "torch.manual_seed(seed_val)\n",
        "torch.cuda.manual_seed_all(seed_val)\n",
        "\n",
        "for epoch in range(num_epochs):\n",
        "\n",
        "  print(\"\\n###################################################\")\n",
        "  print(\"Epoch: {}/{}\".format(epoch+1, num_epochs))\n",
        "  print(\"###################################################\\n\")\n",
        "\n",
        "  #training phase\n",
        "  average_train_loss = 0\n",
        "  average_train_acc = 0\n",
        "  for step, batch in enumerate(train_data_loader):\n",
        "\n",
        "    model.train() \n",
        "    input_ids = batch['input_ids'].to(device)\n",
        "    attention_mask = batch['attention_mask'].to(device)\n",
        "    labels = batch['labels'].to(device)\n",
        "\n",
        "    # set the gradient to zero as by default pytorch is accumulating the gradients\n",
        "    model.zero_grad()\n",
        "\n",
        "    loss, logits = model(input_ids=input_ids,\n",
        "                   attention_mask=attention_mask,\n",
        "                   labels=labels)\n",
        "\n",
        "    #loss is cross entropy loss by default\n",
        "    average_train_loss += loss\n",
        "    \n",
        "    if step % 100 == 0:\n",
        "      print(\"At Step {} Training Loss: {:.5f}\".format(step, loss.item()))\n",
        "\n",
        "    #backpropagation\n",
        "    loss.backward()\n",
        "    #maximum gradient clipping\n",
        "    torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
        "    #update parameters\n",
        "    optimizer.step()\n",
        "    #update learning rate\n",
        "    scheduler.step()\n",
        "\n",
        "    logits_for_acc = logits.detach().cpu().numpy()\n",
        "    label_for_acc = labels.to('cpu').numpy()\n",
        "    average_train_acc += sklearn.metrics.accuracy_score(label_for_acc, np.argmax(logits_for_acc, axis=-1))\n",
        "\n",
        "    #print out sentences + sentiment predictions + labels\n",
        "    #print(tokenizer.batch_decode(input_ids, skip_special_tokens=True))\n",
        "    #print(\"Predictions: \",np.argmax(logits_for_acc, axis=1))\n",
        "    #print(\"Labels:      \",label_for_acc)\n",
        "    #print(\"#############\")\n",
        "      \n",
        "\n",
        "  average_train_loss = average_train_loss / len(train_data_loader)\n",
        "  average_train_acc = average_train_acc / len(train_data_loader)\n",
        "  print(\"======Average Training Loss: {:.5f}=========\".format(average_train_loss))\n",
        "  print(\"======Average Training Accuracy: {:.2f}%========\".format(average_train_acc*100))\n",
        "\n",
        "  #validation phase\n",
        "  average_val_loss = 0\n",
        "  average_val_acc = 0\n",
        "  \n",
        "  for step,batch in enumerate(val_data_loader):\n",
        "    model.eval()\n",
        "    input_ids = batch['input_ids'].to(device)\n",
        "    attention_mask = batch['attention_mask'].to(device)\n",
        "    labels = batch['labels'].to(device)\n",
        "\n",
        "    with torch.no_grad():\n",
        "      loss, logits = model(input_ids=input_ids,\n",
        "                     attention_mask=attention_mask,\n",
        "                     labels=labels)\n",
        "\n",
        "    #loss is cross entropy loss by default\n",
        "    average_val_loss += loss.item()\n",
        "\n",
        "    logits_for_acc = logits.detach().cpu().numpy()\n",
        "    label_for_acc = labels.to('cpu').numpy()\n",
        "    average_val_acc += sklearn.metrics.accuracy_score(label_for_acc, np.argmax(logits_for_acc, axis=-1))\n",
        "\n",
        "    #print out sentences + sentiment predictions + labels\n",
        "    #print(tokenizer.batch_decode(input_ids, skip_special_tokens=True))\n",
        "    #print(\"Predictions: \",np.argmax(logits_for_acc, axis=1))\n",
        "    #print(\"Labels:      \",label_for_acc)\n",
        "    #print(\"#############\")\n",
        "    \n",
        "\n",
        "  average_val_loss = average_val_loss / len(val_data_loader)\n",
        "  average_val_acc = average_val_acc / len(val_data_loader)\n",
        "  print(\"======Average Validation Loss: {:.5f}=========\".format(average_val_loss))\n",
        "  print(\"======Average Validation Accuracy: {:.2f}%======\".format(average_val_acc*100))\n"
      ],
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Some weights of the model checkpoint at bert-base-cased were not used when initializing BertForSequenceClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias']\n",
            "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-cased and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "###################################################\n",
            "Epoch: 1/1\n",
            "###################################################\n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:10: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  # Remove the CWD from sys.path while we load stuff.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "At Step 0 Training Loss: 1.10420\n",
            "At Step 100 Training Loss: 0.89819\n",
            "At Step 200 Training Loss: 0.31455\n",
            "At Step 300 Training Loss: 0.36738\n",
            "At Step 400 Training Loss: 0.59081\n",
            "At Step 500 Training Loss: 0.53577\n",
            "At Step 600 Training Loss: 0.58356\n",
            "At Step 700 Training Loss: 0.32109\n",
            "At Step 800 Training Loss: 0.16012\n",
            "At Step 900 Training Loss: 0.49118\n",
            "At Step 1000 Training Loss: 0.36884\n",
            "At Step 1100 Training Loss: 0.51396\n",
            "======Average Training Loss: 0.64816=========\n",
            "======Average Training Accuracy: 72.66%========\n",
            "======Average Validation Loss: 0.51371=========\n",
            "======Average Validation Accuracy: 80.10%======\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ETcAhcsBlwc9",
        "outputId": "36065d6c-3d05-404d-eb89-661f948a37e9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 226
        }
      },
      "source": [
        "#Following Code is from: https://github.com/huggingface/transformers/blob/35ff345fc9df9e777b27903f11fa213e4052595b/examples/run_glue.py#L495\n",
        "#Saving model\n",
        "\n",
        "output_dir = './model_save/'\n",
        "#Output directory\n",
        "if not os.path.exists(output_dir):\n",
        "    os.makedirs(output_dir)\n",
        "\n",
        "print(\"Saving model to %s\" % output_dir)\n",
        "\n",
        "#Save model and tokenizer\n",
        "model_to_save = model.module if hasattr(model, 'module') else model  # Take care of distributed/parallel training\n",
        "model_to_save.save_pretrained(output_dir)\n",
        "tokenizer.save_pretrained(output_dir)\n",
        "\n"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Saving model to ./model_save/\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-11-6f65a4ee10ae>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0mmodel_to_save\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave_pretrained\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput_dir\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0mtokenizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave_pretrained\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput_dir\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 15\u001b[0;31m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput_dir\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'training_args.bin'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m: name 'args' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3lZMxByLl4N4"
      },
      "source": [
        "#Load trained model\n",
        "\n",
        "model = model_class.from_pretrained(output_dir)\n",
        "tokenizer = tokenizer_class.from_pretrained(output_dir)\n",
        "model.to(device)"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}