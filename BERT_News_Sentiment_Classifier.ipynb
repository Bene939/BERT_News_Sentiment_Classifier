{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Kopie von BERT_News_Sentiment_Classifier.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Bene939/BERT_News_Sentiment_Classifier/blob/main/BERT_News_Sentiment_Classifier.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bdtwSL6npW25",
        "outputId": "96ebe9d8-3fc9-4f33-eb80-b10ce7af3e8a",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "!pip install transformers\n",
        "!pip install torch\n",
        "!pip install pandas\n",
        "!pip install pathlib\n",
        "!pip install sklearn\n",
        "!pip install numpy"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting transformers\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/9c/34/fb092588df61bf33f113ade030d1cbe74fb73a0353648f8dd938a223dce7/transformers-3.5.0-py3-none-any.whl (1.3MB)\n",
            "\u001b[K     |████████████████████████████████| 1.3MB 13.6MB/s \n",
            "\u001b[?25hCollecting sentencepiece==0.1.91\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/d4/a4/d0a884c4300004a78cca907a6ff9a5e9fe4f090f5d95ab341c53d28cbc58/sentencepiece-0.1.91-cp36-cp36m-manylinux1_x86_64.whl (1.1MB)\n",
            "\u001b[K     |████████████████████████████████| 1.1MB 50.9MB/s \n",
            "\u001b[?25hCollecting sacremoses\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/7d/34/09d19aff26edcc8eb2a01bed8e98f13a1537005d31e95233fd48216eed10/sacremoses-0.0.43.tar.gz (883kB)\n",
            "\u001b[K     |████████████████████████████████| 890kB 62.0MB/s \n",
            "\u001b[?25hCollecting tokenizers==0.9.3\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/4c/34/b39eb9994bc3c999270b69c9eea40ecc6f0e97991dba28282b9fd32d44ee/tokenizers-0.9.3-cp36-cp36m-manylinux1_x86_64.whl (2.9MB)\n",
            "\u001b[K     |████████████████████████████████| 2.9MB 52.1MB/s \n",
            "\u001b[?25hRequirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.6/dist-packages (from transformers) (2019.12.20)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.6/dist-packages (from transformers) (3.0.12)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.6/dist-packages (from transformers) (20.4)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from transformers) (1.18.5)\n",
            "Requirement already satisfied: protobuf in /usr/local/lib/python3.6/dist-packages (from transformers) (3.12.4)\n",
            "Requirement already satisfied: dataclasses; python_version < \"3.7\" in /usr/local/lib/python3.6/dist-packages (from transformers) (0.7)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.6/dist-packages (from transformers) (4.41.1)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.6/dist-packages (from transformers) (2.23.0)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers) (1.15.0)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers) (7.1.2)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers) (0.17.0)\n",
            "Requirement already satisfied: pyparsing>=2.0.2 in /usr/local/lib/python3.6/dist-packages (from packaging->transformers) (2.4.7)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.6/dist-packages (from protobuf->transformers) (50.3.2)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (2020.6.20)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (3.0.4)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (1.24.3)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (2.10)\n",
            "Building wheels for collected packages: sacremoses\n",
            "  Building wheel for sacremoses (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for sacremoses: filename=sacremoses-0.0.43-cp36-none-any.whl size=893257 sha256=dbb0285078e26135efe08e8b72dcb086909b46714f9fe41828cfac77c00a7402\n",
            "  Stored in directory: /root/.cache/pip/wheels/29/3c/fd/7ce5c3f0666dab31a50123635e6fb5e19ceb42ce38d4e58f45\n",
            "Successfully built sacremoses\n",
            "Installing collected packages: sentencepiece, sacremoses, tokenizers, transformers\n",
            "Successfully installed sacremoses-0.0.43 sentencepiece-0.1.91 tokenizers-0.9.3 transformers-3.5.0\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.6/dist-packages (1.7.0+cu101)\n",
            "Requirement already satisfied: dataclasses in /usr/local/lib/python3.6/dist-packages (from torch) (0.7)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from torch) (1.18.5)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.6/dist-packages (from torch) (3.7.4.3)\n",
            "Requirement already satisfied: future in /usr/local/lib/python3.6/dist-packages (from torch) (0.16.0)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.6/dist-packages (1.1.4)\n",
            "Requirement already satisfied: numpy>=1.15.4 in /usr/local/lib/python3.6/dist-packages (from pandas) (1.18.5)\n",
            "Requirement already satisfied: python-dateutil>=2.7.3 in /usr/local/lib/python3.6/dist-packages (from pandas) (2.8.1)\n",
            "Requirement already satisfied: pytz>=2017.2 in /usr/local/lib/python3.6/dist-packages (from pandas) (2018.9)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.6/dist-packages (from python-dateutil>=2.7.3->pandas) (1.15.0)\n",
            "Requirement already satisfied: pathlib in /usr/local/lib/python3.6/dist-packages (1.0.1)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "58IV4lV7plhd"
      },
      "source": [
        "from transformers import BertModel, DistilBertModel, BertForSequenceClassification, AdamW, BertTokenizer, get_linear_schedule_with_warmup, Trainer, TrainingArguments\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.utils.data import DataLoader, RandomSampler, SequentialSampler, TensorDataset\n",
        "import pandas as pd\n",
        "from pathlib import Path\n",
        "import sklearn\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score, precision_recall_fscore_support\n",
        "import numpy as np\n",
        "from torch.nn import functional as F\n",
        "from collections import defaultdict\n",
        "import random"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "txYiiXGwrjN2"
      },
      "source": [
        "# setting device as gpu or cpu\n",
        "\n",
        "if torch.cuda.is_available():\n",
        "  print(\"\\nUsing: \", torch.cuda.get_device_name(0))\n",
        "  device = torch.device('cuda')\n",
        "else:\n",
        "  print(\"\\nUsing: CPU\")\n",
        "  device = torch.device('cpu')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nGvkz7wTC3W0"
      },
      "source": [
        "#TODO: compare balanced vs unbalanced dataset performance (might not be necessary: pretty balanced already)\n",
        "#TODO: balancing dataset with pandas\n",
        "\n",
        "# loading dataset\n",
        "labeled_dataset = \"news_headlines_sentiment.csv\"\n",
        "labeled_dataset_file = Path(labeled_dataset)\n",
        "file_loaded = False\n",
        "while not file_loaded:\n",
        "  if labeled_dataset_file.exists():\n",
        "    labeled_dataset = pd.read_csv(labeled_dataset_file)\n",
        "    file_loaded = True\n",
        "    print(\"Dataset Loaded\")\n",
        "  else:\n",
        "    print(\"File not Found\")\n",
        "print(labeled_dataset)\n",
        "print(labeled_dataset.sentiment.value_counts())\n",
        "\"\"\"\n",
        "\n",
        "# balancing dataset to 1/3 per sentiment\n",
        "for idx, row in labeled_dataset.iterrows():\n",
        "  if row[\"sentiment\"] == 0:\n",
        "    if negative - neutral != 0:\n",
        "      index_name = labeled_dataset[labeled_dataset[\"news\"] == row[\"news\"]].index\n",
        "      labeled_dataset.drop(index_name, inplace=True)\n",
        "      negative -= 1\n",
        "  elif row[\"sentiment\"] == 2:\n",
        "    if positive - neutral != 0:\n",
        "      index_name = labeled_dataset[labeled_dataset[\"news\"] == row[\"news\"]].index\n",
        "      labeled_dataset.drop(index_name, inplace=True)\n",
        "      positive -= 1\n",
        "\"\"\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xatcG9i7bgBh"
      },
      "source": [
        "# loading phrase bank dataset and correcting format\n",
        "\n",
        "phrase_bank_dataset = \"all-data.csv\"\n",
        "phrase_bank_dataset_file = Path(phrase_bank_dataset)\n",
        "file_loaded = False\n",
        "while not file_loaded:\n",
        "  if phrase_bank_dataset_file.exists():\n",
        "    phrase_dataset = pd.read_csv(phrase_bank_dataset, encoding='latin-1', names=[\"sentiment\", \"news\"])\n",
        "    phrase_dataset = phrase_dataset[[\"news\", \"sentiment\"]]\n",
        "    phrase_dataset[\"sentiment\"].replace(['positive', 'negative', 'neutral'], [0,1,2], inplace=True)\n",
        "    file_loaded = True\n",
        "    print(\"Dataset Loaded\")\n",
        "  else:\n",
        "    print(\"File not Found\")\n",
        "print(phrase_dataset)\n",
        "print(phrase_dataset.sentiment.value_counts())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4y2v0HmPEqrY"
      },
      "source": [
        "# merge both datasets\n",
        "merged_dataset = pd.concat([phrase_dataset, labeled_dataset], axis=0)\n",
        "print(merged_dataset)\n",
        "print(merged_dataset.sentiment.value_counts())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lT0KNTS5NtT1"
      },
      "source": [
        "# custom dataset class which returns the encodings and labels when called by the data loader\n",
        "# code is from https://huggingface.co/transformers/custom_datasets.html\n",
        "\n",
        "class NewsSentimentDataset(torch.utils.data.Dataset):\n",
        "  def __init__(self, encodings, labels):\n",
        "        self.encodings = encodings\n",
        "        self.labels = labels\n",
        "\n",
        "  def __getitem__(self, idx):\n",
        "      item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
        "      item['labels'] = torch.tensor(self.labels[idx])\n",
        "      return item\n",
        "\n",
        "  def __len__(self):\n",
        "      return len(self.labels)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5eQ2BqPNNQhT"
      },
      "source": [
        "# expects data frame and tokenizer as input\n",
        "# returns encoded data as NewsSentimentDataset\n",
        "# token type ids are not included in encoding as they are only for Q&A to mark if its a question or an answer\n",
        "\n",
        "def tokenize_headlines(df, tokenizer):\n",
        "  encodings = tokenizer.batch_encode_plus(\n",
        "      df[\"news\"].tolist(),           # input the news headlines\n",
        "      add_special_tokens = True,     # special tokens added to mark beginning & end of sentence\n",
        "      truncation = True,             # make all sentences a fixed length\n",
        "      padding = 'max_length',        # pad with zeros to max length\n",
        "      return_attention_mask = True,  # include attention mask in encoding\n",
        "      return_tensors = 'pt'          # return as pytorch tensor\n",
        "  )\n",
        "\n",
        "  dataset = NewsSentimentDataset(encodings, df[\"sentiment\"].tolist())\n",
        "  return dataset"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wqAQGCeVOlvh"
      },
      "source": [
        "#splitting dataset into training and validation set\n",
        "#load news sentiment dataset\n",
        "#defining tokenizer\n",
        "tokenizer = BertTokenizer.from_pretrained('bert-base-cased')\n",
        "train_data, val_data = train_test_split(merged_dataset, test_size=.2)\n",
        "\n",
        "print(\"Train Dataset\\n\", train_data.reset_index(drop=True))\n",
        "print(\"Validation Dataset\\n\", val_data.reset_index(drop=True))\n",
        "\n",
        "train_dataset = tokenize_headlines(train_data, tokenizer)\n",
        "val_dataset = tokenize_headlines(val_data, tokenizer)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "I23_etTkePaN"
      },
      "source": [
        "#TODO: Calculate per class accuracy\n",
        "#TODO: Try out different Loss function\n",
        "#TODO: Optimizer zero grad etc necessary?\n",
        "#TODO: Cased vs Uncased\n",
        "\n",
        "model = BertForSequenceClassification.from_pretrained('bert-base-cased', num_labels=3)\n",
        "model = model.to(device)\n",
        "#data loader\n",
        "train_batch_size = 8\n",
        "val_batch_size = 8\n",
        "\n",
        "train_data_loader = DataLoader(train_dataset, batch_size = train_batch_size, sampler=RandomSampler(train_dataset))\n",
        "val_data_loader = DataLoader(val_dataset, batch_size = val_batch_size, sampler=SequentialSampler(val_dataset))\n",
        "\n",
        "#optimizer and scheduler\n",
        "num_epochs = 1\n",
        "num_steps = len(train_data_loader) * num_epochs\n",
        "optimizer = AdamW(model.parameters(), lr=5e-5, eps=1e-8)\n",
        "scheduler = get_linear_schedule_with_warmup(optimizer, num_warmup_steps=1000, num_training_steps=num_steps)\n",
        "\n",
        "#training and evaluation\n",
        "seed_val = 64\n",
        "random.seed(seed_val)\n",
        "np.random.seed(seed_val)\n",
        "torch.manual_seed(seed_val)\n",
        "torch.cuda.manual_seed_all(seed_val)\n",
        "\n",
        "for epoch in range(num_epochs):\n",
        "\n",
        "  print(\"\\n###################################################\")\n",
        "  print(\"Epoch: {}/{}\".format(epoch+1, num_epochs))\n",
        "  print(\"###################################################\\n\")\n",
        "\n",
        "  #training phase\n",
        "  average_train_loss = 0\n",
        "  average_train_acc = 0\n",
        "  for step, batch in enumerate(train_data_loader):\n",
        "\n",
        "    model.train() \n",
        "    input_ids = batch['input_ids'].to(device)\n",
        "    attention_mask = batch['attention_mask'].to(device)\n",
        "    labels = batch['labels'].to(device)\n",
        "\n",
        "    # set the gradient to zero as by default pytorch is accumulating the gradients\n",
        "    model.zero_grad()\n",
        "    optimizer.zero_grad()\n",
        "\n",
        "    loss, logits = model(input_ids=input_ids,\n",
        "                   attention_mask=attention_mask,\n",
        "                   labels=labels)\n",
        "\n",
        "    #loss = F.cross_entropy(logits, labels)\n",
        "    average_train_loss += loss\n",
        "    \n",
        "    if step % 100 == 0:\n",
        "      print(\"At Step {} Training Loss: {:.5f}\".format(step, loss.item()))\n",
        "\n",
        "    \n",
        "    loss.backward()\n",
        "    #maximum gradient clipping\n",
        "    torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
        "    \n",
        "    optimizer.step()\n",
        "    scheduler.step()\n",
        "    model.zero_grad()\n",
        "\n",
        "    logits_for_acc = logits.detach().cpu().numpy()\n",
        "    label_for_acc = labels.to('cpu').numpy()\n",
        "    average_train_acc += sklearn.metrics.accuracy_score(label_for_acc, np.argmax(logits_for_acc, axis=-1))\n",
        "\n",
        "    #print out sentences + sentiment predictions + labels\n",
        "    #print(tokenizer.batch_decode(input_ids, skip_special_tokens=True))\n",
        "    #print(\"Predictions: \",np.argmax(logits_for_acc, axis=1))\n",
        "    #print(\"Labels:      \",label_for_acc)\n",
        "    #print(\"#############\")\n",
        "      \n",
        "\n",
        "  average_train_loss = average_train_loss / len(train_data_loader)\n",
        "  average_train_acc = average_train_acc / len(train_data_loader)\n",
        "  print(\"======Average Training Loss: {:.5f}=========\".format(average_train_loss))\n",
        "  print(\"======Average Training Accuracy: {:.2f}%========\".format(average_train_acc*100))\n",
        "\n",
        "  #validation phase\n",
        "  average_val_loss = 0\n",
        "  average_val_acc = 0\n",
        "  \n",
        "  for step,batch in enumerate(val_data_loader):\n",
        "    model.eval()\n",
        "    input_ids = batch['input_ids'].to(device)\n",
        "    attention_mask = batch['attention_mask'].to(device)\n",
        "    labels = batch['labels'].to(device)\n",
        "\n",
        "    with torch.no_grad():\n",
        "      loss, logits = model(input_ids=input_ids,\n",
        "                     attention_mask=attention_mask,\n",
        "                     labels=labels)\n",
        "\n",
        "    #loss = F.cross_entropy(logits, labels)\n",
        "    average_val_loss += loss.item()\n",
        "\n",
        "    logits_for_acc = logits.detach().cpu().numpy()\n",
        "    label_for_acc = labels.to('cpu').numpy()\n",
        "    #print out sentences + sentiment predictions + labels\n",
        "    #print(tokenizer.batch_decode(input_ids, skip_special_tokens=True))\n",
        "    #print(\"Predictions: \",np.argmax(logits_for_acc, axis=1))\n",
        "    #print(\"Labels:      \",label_for_acc)\n",
        "    #print(\"#############\")\n",
        "    average_val_acc += sklearn.metrics.accuracy_score(label_for_acc, np.argmax(logits_for_acc, axis=-1))\n",
        "    \n",
        "\n",
        "  average_val_loss = average_val_loss / len(val_data_loader)\n",
        "  average_val_acc = average_val_acc / len(val_data_loader)\n",
        "  print(\"======Average Validation Loss: {:.5f}=========\".format(average_val_loss))\n",
        "  print(\"======Average Validation Accuracy: {:.2f}%======\".format(average_val_acc*100))\n"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}